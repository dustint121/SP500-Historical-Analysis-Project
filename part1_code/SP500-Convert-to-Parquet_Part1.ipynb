{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%stop_session\n%idle_timeout 15\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom pyspark.sql.functions import *\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.job import Job\n\nimport boto3 \nfrom pyspark.sql.functions import monotonically_increasing_id \nfrom pyspark.sql.window import Window \nfrom pyspark.sql.functions import row_number\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nThere is no current session.\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 15 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 15\nSession ID: d7a738b9-7dc8-4e7e-b2e2-ea6ce7c2858d\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\nWaiting for session d7a738b9-7dc8-4e7e-b2e2-ea6ce7c2858d to get into ready status...\nSession d7a738b9-7dc8-4e7e-b2e2-ea6ce7c2858d has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Handle Data Ranges file",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "s3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nfolder_path = 'SP500_date_ranges.csv'\n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{folder_path}\"]}, \n                                                    format=\"csv\",\n                                                    format_options={\"withHeader\": True})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert DynamicFrame to DataFrame \ndf = dyf_original.toDF() \n\ndf = df.withColumn(\"date_range_start\", col(\"date_range_start\").cast(\"date\"))\ndf = df.withColumn(\"date_range_end\", col(\"date_range_end\").cast(\"date\"))\n\n# Add index column \nwindow = Window.orderBy(monotonically_increasing_id()) \ndf = df.withColumn(\"index\", row_number().over(window))\n\ndyf_with_index = DynamicFrame.fromDF(df, glueContext, \"dyf_with_index\") \ndyf_with_index.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date_range_start: date\n|-- date_range_end: date\n|-- index: int\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3 = boto3.client('s3')\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_date_ranges/'\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_date_ranges/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_date_ranges/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db\", catalogTableName=\"PARQUET_date_ranges\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf_with_index)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fc7f0af4430>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Handle Company Profiles",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Define S3 bucket and path \nbucket_name = 'sp500-historical-analysis-project' \nfolder_path = 'local_data/company_profiles/' \n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{folder_path}\"], \"recurse\": True}, \n                                                    format=\"json\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df = dyf_original.toDF() #convert Glue DF to PySpark DF\n\n#get relevant data from file name\nspark_df = spark_df.withColumn('S3_filename_path', input_file_name()) #get original file location for data\nspark_df = spark_df.withColumn(\"filename\", split(col('S3_filename_path'), \"/\").getItem(5))\nspark_df = spark_df.withColumn(\"temp\", split(col(\"filename\"), \".json\").getItem(0)) \nspark_df = spark_df.withColumn(\"index\", split(col(\"temp\"), \"_\").getItem(0)) \nspark_df = spark_df.withColumn(\"index\", col(\"index\").cast(\"int\"))\nspark_df = spark_df.drop(\"temp\", \"filename\", \"S3_filename_path\")\n\nspark_df = spark_df.orderBy(\"index\") \nspark_df.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+--------------------+-----------+--------------------+--------+------------------+-----------------+------+-----+\n|ticker|        company_name|is_delisted|         description|exchange|            sector|         industry|source|index|\n+------+--------------------+-----------+--------------------+--------+------------------+-----------------+------+-----+\n|  DELL|Dell Technologies...|      false|Dell Technologies...|    NYSE|        Technology|Computer Hardware|tiingo|    0|\n|  ERIE|Erie Indemnity Co...|      false|Erie Indemnity Co...|  NASDAQ|Financial Services|Insurance Brokers|tiingo|    1|\n+------+--------------------+-----------+--------------------+--------+------------------+-----------------+------+-----+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.count(), dyf_original.count() #should match",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "(1153, 1153)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from io import StringIO\nfrom datetime import datetime\nimport pandas as pd\ns3 = boto3.client('s3')\n# Define the bucket name and the file key (path to your CSV file in the bucket)\nbucket_name = 'sp500-historical-analysis-project'\nfile_key = 'cleaned_sp_500_dataset.csv'\nresponse = s3.get_object(Bucket=bucket_name, Key=file_key) # Get the object from S3\ncsv_content = response['Body'].read().decode('utf-8') # Read the file content as a string\ncsv_buffer = StringIO(csv_content) # Use StringIO to convert the string data into a file-like object\n\ndf = pd.read_csv(csv_buffer).iloc[:1153]\ndf[\"id\"] = [i for i in range(len(df))]\ndf[\"Removed_Date\"] = df[\"Removed_Date\"].fillna(\"September 30, 2024\")\ndf = df.where(pd.notnull(df), None) #replace \"NAN\" values with NULL\n\ndf[\"Added_Date\"] = df[\"Added_Date\"].apply(lambda x: (datetime.strptime(x, \"%B %d, %Y\")).__str__()[:10])\ndf[\"Removed_Date\"] = df[\"Removed_Date\"].apply(lambda x: (datetime.strptime(x, \"%B %d, %Y\")).__str__()[:10])\n\ndf = df.drop(columns=['Removal_Reason', 'Replaces', 'Ticker', 'Name'])\ndf.head() # Display the DataFrame",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 125,
			"outputs": [
				{
					"name": "stdout",
					"text": "   Added_Date Removed_Date  id\n0  2024-09-23   2024-09-30   0\n1  2024-09-23   2024-09-30   1\n2  2024-09-23   2024-09-30   2\n3  2024-07-05   2024-09-30   3\n4  2024-06-24   2024-09-30   4\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "temp = spark.createDataFrame(df)\njoined_df = spark_df.join(temp, spark_df[\"Index\"] == temp[\"id\"], \"inner\")\n\ntemp.count(), spark_df.count(), joined_df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 126,
			"outputs": [
				{
					"name": "stdout",
					"text": "(1153, 1153, 1153)\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "joined_df = joined_df.withColumn(\"added_date\", col(\"Added_Date\").cast(\"date\"))\njoined_df = joined_df.withColumn(\"removal_date\", col(\"Removed_Date\").cast(\"date\"))\njoined_df = joined_df.withColumn(\"company_details\", col(\"description\").cast(\"string\"))\njoined_df = joined_df.drop('description','id','Name','Removed_Date') #Note: capitalization is not considered by spark\njoined_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 127,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- ticker: string (nullable = true)\n |-- company_name: string (nullable = true)\n |-- is_delisted: boolean (nullable = true)\n |-- exchange: string (nullable = true)\n |-- sector: string (nullable = true)\n |-- industry: string (nullable = true)\n |-- source: string (nullable = true)\n |-- index: integer (nullable = true)\n |-- added_date: date (nullable = true)\n |-- removal_date: date (nullable = true)\n |-- company_details: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "joined_df = joined_df.orderBy(\"index\") \njoined_df.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 128,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+--------------------+-----------+--------+------------------+-----------------+------+-----+----------+------------+--------------------+\n|ticker|        company_name|is_delisted|exchange|            sector|         industry|source|index|added_date|removal_date|     company_details|\n+------+--------------------+-----------+--------+------------------+-----------------+------+-----+----------+------------+--------------------+\n|  DELL|Dell Technologies...|      false|    NYSE|        Technology|Computer Hardware|tiingo|    0|2024-09-23|  2024-09-30|Dell Technologies...|\n|  ERIE|Erie Indemnity Co...|      false|  NASDAQ|Financial Services|Insurance Brokers|tiingo|    1|2024-09-23|  2024-09-30|Erie Indemnity Co...|\n+------+--------------------+-----------+--------+------------------+-----------------+------+-----+----------+------------+--------------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(joined_df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 133,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- ticker: string\n|-- company_name: string\n|-- is_delisted: boolean\n|-- exchange: string\n|-- sector: string\n|-- industry: string\n|-- source: string\n|-- index: int\n|-- added_date: date\n|-- removal_date: date\n|-- company_details: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\ns3 = boto3.client('s3')\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_company_profiles/'\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 135,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_company_profiles/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_company_profiles/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db\", catalogTableName=\"PARQUET_company_profiles\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 136,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f78b7c20bb0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Handle Market Cap Metadata",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Define S3 bucket and path \nbucket_name = 'sp500-historical-analysis-project' \nfolder_path = 'local_data/market_cap_metadata//' \n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{folder_path}\"], \"recurse\": True}, \n                                                    format=\"json\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = dyf_original.toDF() #convert Glue DF to PySpark DF\ndf = df.orderBy(\"index\") \ndf.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----+------+------+------------------------+-----------------------+----------------+----------+-----------------------+--------+----------------------+-----------------------------+\n|index|ticker|source|first_day_have_vs_needed|last_day_have_vs_needed|num_of_days_data|image_type|missing_num_days_before|is_empty|missing_num_days_after|num_trading_days_to_calculate|\n+-----+------+------+------------------------+-----------------------+----------------+----------+-----------------------+--------+----------------------+-----------------------------+\n|    0|  DELL|tiingo|    2024-09-23 : 2024...|   2024-09-30 : 2024...|               6|      null|                   null|    null|                  null|                         null|\n|    1|  ERIE|tiingo|    2024-09-23 : 2024...|   2024-09-30 : 2024...|               6|      null|                   null|    null|                  null|                         null|\n+-----+------+------+------------------------+-----------------------+----------------+----------+-----------------------+--------+----------------------+-----------------------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.count(), dyf_original.count() #should match",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 53,
			"outputs": [
				{
					"name": "stdout",
					"text": "(1153, 1153)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- index: int\n|-- ticker: string\n|-- source: string\n|-- first_day_have_vs_needed: string\n|-- last_day_have_vs_needed: string\n|-- num_of_days_data: int\n|-- image_type: string\n|-- missing_num_days_before: int\n|-- is_empty: boolean\n|-- missing_num_days_after: int\n|-- num_trading_days_to_calculate: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_market_cap_metadata/'\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_market_cap_metadata/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_market_cap_metadata/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db\", catalogTableName=\"PARQUET_market_cap_metadata\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f7ad2a9ccd0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Handle Market Cap Data",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "#using glueContext.create_dynamic_frame.from_options() loses json data (3310722 vs 3311837 expected)\ndf = spark.read.option(\"multiline\", \"true\").json(\"s3://sp500-historical-analysis-project/local_data/company_market_cap_data/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#get relevant data from file name\ndf = df.withColumn('S3_filename_path', input_file_name()) #get original file location for data\ndf = df.withColumn(\"filename\", split(col('S3_filename_path'), \"/\").getItem(5))\ndf = df.withColumn(\"temp\", split(col(\"filename\"), \".json\").getItem(0)) \ndf = df.withColumn(\"index\", split(col(\"temp\"), \"_\").getItem(0)) \ndf = df.withColumn(\"index\", col(\"index\").cast(\"int\"))\ndf = df.withColumn(\"ticker\", split(col(\"temp\"), \"_\").getItem(1))\ndf = df.drop(\"temp\", \"filename\", \"S3_filename_path\")\n\n#handle dates\ndf = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\")) # Convert to date type \n# Extract year, month, and day \ndf = df.withColumn(\"year\", year(df[\"date\"])) \ndf = df.withColumn(\"month\", month(df[\"date\"])) \ndf = df.withColumn(\"day\", dayofmonth(df[\"date\"]))\n\n#sort the spark df by date; this will cut writing time by over 90%!!!\ndf = df.orderBy(\"year\", \"month\", \"day\", \"index\") \ndf.show(5)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------------+-----+------+----+-----+---+\n|      date|     market_cap|index|ticker|year|month|day|\n+----------+---------------+-----+------+----+-----+---+\n|1998-01-02|7.22524517309E9|  117|   BKR|1998|    1|  2|\n|1998-01-02|  7.897793176E9|  133|     D|1998|    1|  2|\n|1998-01-02|3.46196694369E9|  191|   HUM|1998|    1|  2|\n|1998-01-02|3.08283962778E9|  208|   FCX|1998|    1|  2|\n|1998-01-02|6.48851865473E9|  284|   TMO|1998|    1|  2|\n+----------+---------------+-----+------+----+-----+---+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.where(df[\"index\"] == 0).show() #needs 6 rows",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-----------------+-----+------+----+-----+---+\n|      date|       market_cap|index|ticker|year|month|day|\n+----------+-----------------+-----+------+----+-----+---+\n|2024-09-23|8.339288691681E10|    0|  DELL|2024|    9| 23|\n|2024-09-24|8.320846784223E10|    0|  DELL|2024|    9| 24|\n|2024-09-25|8.523707766261E10|    0|  DELL|2024|    9| 25|\n|2024-09-26|8.971987978317E10|    0|  DELL|2024|    9| 26|\n|2024-09-27|8.527254286926E10|    0|  DELL|2024|    9| 27|\n|2024-09-30|8.408091192582E10|    0|  DELL|2024|    9| 30|\n+----------+-----------------+-----+------+----+-----+---+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#should match \n#expected number based on local machine testing: 3311837\ndf.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "3311837\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date: date\n|-- market_cap: double\n|-- index: int\n|-- ticker: string\n|-- year: int\n|-- month: int\n|-- day: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df_sample = df.sample(0.1)\n# print(df_sample.count())\n# dyf = DynamicFrame.fromDF(df_sample, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \n# dyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_company_market_cap_data/'\n\n\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\n\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 356 objects from sp500-historical-analysis-project/PARQUET_company_market_cap_data/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_company_market_cap_data/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[\"year\", \"month\"],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db\", catalogTableName=\"PARQUET_marketcaps\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f92772fe170>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 95da716c-c609-4830-b67c-2384eff6d6e3\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Times for Converting Marketcap Data and Writing As Parquet files to S3",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Times with 10 workers without sorting\n* 300 rows     : 11 seconds \n* 3000 rows    : 27 seconds \n* 30000 rows   : 185 seconds (3 minutes)\n* 300000 rows  : 20+ minutes",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "#### Times with 30 workers without sorting\n* 3000 rows    : 18 seconds \n* 30000 rows   : 154 seconds (3 minutes)\n* 300000 rows  : 1174 seconds (20 minutes)",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "#### Times with 10 workers and Pre-Sorting of Spark DF by partitions (df.orderby)\n* 3000000 rows : 16 seconds",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "#### Times with 30 workers and Pre-Sorting of Spark DF by partitions (df.orderby)\n* 300000 rows  : 34 seconds\n* 3000000 rows : 37 seconds",
			"metadata": {
				"tags": []
			}
		}
	]
}
{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "**Note** : In AWS Athena, you must make database, sp500_db_final, with the following command in query editor first:\n>CREATE DATABASE sp500_db_final",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%stop_session\n%idle_timeout 15\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom pyspark.sql.functions import *\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.job import Job\n\nimport boto3 \nfrom pyspark.sql.functions import monotonically_increasing_id \nfrom pyspark.sql.window import Window \nfrom pyspark.sql.functions import row_number\nfrom pyspark.sql.types import DecimalType\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 0fc21c48-db9f-4b09-ac56-62b47cebcfc1\nStopped session.\nCurrent idle_timeout is 15 minutes.\nidle_timeout has been set to 15 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 15\nSession ID: 4c9ecdce-f987-4e3d-b8f5-3d2382df7244\nApplying the following default arguments:\n--glue_kernel_version 1.0.10\n--enable-glue-datacatalog true\nWaiting for session 4c9ecdce-f987-4e3d-b8f5-3d2382df7244 to get into ready status...\nSession 4c9ecdce-f987-4e3d-b8f5-3d2382df7244 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Final - Handle SP500 Market Cap Estimations",
			"metadata": {
				"jp-MarkdownHeadingCollapsed": true,
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Define S3 bucket and path \nbucket_name = 'sp500-historical-analysis-project' \nfile_path = 'final_data/sp500_total_market_cap.csv' \n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{file_path}\"]}, \n                                                    format=\"csv\",\n                                                    format_options={'withHeader': True})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf_original.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- Date: string\n|-- Closing Index Value: string\n|-- Total Market Capitalization: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = dyf_original.toDF() #convert Glue DF to PySpark DF\n\n#handle dates\ndf = df.withColumn(\"Date\", to_date(df[\"Date\"], \"yyyy-MM-dd\")) # Convert to date type \ndf = df.withColumnRenamed(\"Date\", \"date\")\n# Extract year, month, and day \ndf = df.withColumn(\"year\", year(df[\"date\"])) \ndf = df.withColumn(\"month\", month(df[\"date\"])) \ndf = df.withColumn(\"day\", dayofmonth(df[\"date\"]))\n\n\ndf = df.withColumn(\"Closing Index Value\", col(\"Closing Index Value\").cast(DecimalType(20, 2)))\ndf = df.withColumn(\"Total Market Capitalization\", col(\"Total Market Capitalization\").cast(DecimalType(20, 2)))\n\ndf = df.orderBy(\"date\")\ndf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- date: date (nullable = true)\n |-- Closing Index Value: decimal(20,2) (nullable = true)\n |-- Total Market Capitalization: decimal(20,2) (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.count() #7043 rows expected",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "7043\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date: date\n|-- Closing Index Value: decimal\n|-- Total Market Capitalization: decimal\n|-- year: int\n|-- month: int\n|-- day: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_sp500_daily_expectations_from_index_values'\n\n\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\n\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_sp500_daily_expectations_from_index_values\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_sp500_daily_expectations_from_index_values/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db_final\", catalogTableName=\"PARQUET_sp500_marketcap_expectations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7fb864daee30>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Final - Handle Trading Days",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Define S3 bucket and path \nbucket_name = 'sp500-historical-analysis-project' \nfile_path = 'final_data/trading_days_with_constituents.csv' \n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{file_path}\"]}, \n                                                    format=\"csv\",\n                                                    format_options={'withHeader': True})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = dyf_original.toDF() #convert Glue DF to PySpark DF\n\n#handle dates\ndf = df.withColumn(\"trading_date\", to_date(df[\"trading_date\"], \"yyyy-MM-dd\")) # Convert to date type \n# Extract year, month, and day \ndf = df.withColumn(\"year\", year(df[\"trading_date\"])) \ndf = df.withColumn(\"month\", month(df[\"trading_date\"])) \ndf = df.withColumn(\"day\", dayofmonth(df[\"trading_date\"]))\ndf = df.withColumn(\"num_constituents\", col(\"number_of_constituents\").cast(\"int\"))\ndf = df.drop(\"date_range_start\", \"date_range_end\", \"number_of_constituents\")\ndf = df.orderBy(\"trading_date\")\ndf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- trading_date: date (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- num_constituents: integer (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# df.filter(col(\"num_constituents\").isNull()).show()\n# df.filter(col(\"trading_date\").isNull()).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "df.count() ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "7044\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- trading_date: date\n|-- year: int\n|-- month: int\n|-- day: int\n|-- num_constituents: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_final_trading_days/'\n\n\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\n\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_final_trading_days/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_final_trading_days/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db_final\", catalogTableName=\"PARQUET_trading_days\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f9293819cc0>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Final - Handle Company Profiles",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Define S3 bucket and path \nbucket_name = 'sp500-historical-analysis-project' \nfolder_path = 'final_data/company_profiles/' \n\n# Create DynamicFrame from JSON files in S3 \ndyf_original = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", \n                                                    connection_options={\"paths\": [f\"s3://{bucket_name}/{folder_path}\"], \"recurse\": True}, \n                                                    format=\"json\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df = dyf_original.toDF() #convert Glue DF to PySpark DF\n\n#get relevant data from file name\nspark_df = spark_df.withColumn('S3_filename_path', input_file_name()) #get original file location for data\nspark_df = spark_df.withColumn(\"filename\", split(col('S3_filename_path'), \"/\").getItem(5))\nspark_df = spark_df.withColumn(\"temp\", split(col(\"filename\"), \".json\").getItem(0)) \nspark_df = spark_df.withColumn(\"index\", split(col(\"temp\"), \"_\").getItem(0)) \nspark_df = spark_df.withColumn(\"index\", col(\"index\").cast(\"int\"))\nspark_df = spark_df.drop(\"temp\", \"filename\", \"S3_filename_path\")\n\nspark_df = spark_df.withColumn(\"company_details\", col(\"description\").cast(\"string\"))\nspark_df = spark_df.drop('description') #Note: capitalization is not considered by spark\n\nspark_df = spark_df.orderBy(\"index\") \nspark_df.show(2)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------+------------+-----------------+--------------------+-----------+------+--------+-----+--------------------+\n|ticker|company_name|           sector|            industry|is_delisted|source|exchange|index|     company_details|\n+------+------------+-----------------+--------------------+-----------+------+--------+-----+--------------------+\n|   CRH|     CRH plc|  Basic Materials|Construction Mate...|      false|   fmp|    NYSE|    0|CRH plc, together...|\n|  CVNA| Carvana Co.|Consumer Cyclical|  Auto - Dealerships|      false|   fmp|    NYSE|    1|Carvana Co., toge...|\n+------+------------+-----------------+--------------------+-----------+------+--------+-----+--------------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.count(), dyf_original.count() #should match",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "(1177, 1177)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- ticker: string (nullable = true)\n |-- company_name: string (nullable = true)\n |-- sector: string (nullable = true)\n |-- industry: string (nullable = true)\n |-- is_delisted: boolean (nullable = true)\n |-- source: string (nullable = true)\n |-- exchange: string (nullable = true)\n |-- index: integer (nullable = true)\n |-- company_details: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(spark_df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- ticker: string\n|-- company_name: string\n|-- sector: string\n|-- industry: string\n|-- is_delisted: boolean\n|-- source: string\n|-- exchange: string\n|-- index: int\n|-- company_details: string\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_final_company_profiles/'\n\n\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\n\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_final_company_profiles/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_final_company_profiles/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db_final\", catalogTableName=\"PARQUET_company_profiles\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f92930bf850>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Handle Market Cap Data",
			"metadata": {
				"tags": [],
				"jp-MarkdownHeadingCollapsed": true
			}
		},
		{
			"cell_type": "code",
			"source": "#using glueContext.create_dynamic_frame.from_options() loses json data (3310722 vs 3311837 expected)\n#about 2-3 minutes to run\nraw_df = spark.read.option(\"multiline\", \"true\").json(\"s3://sp500-historical-analysis-project/final_data/company_market_cap_data/\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "raw_df.count() #3,469,774 rows",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "3469774\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#get relevant data from file name\ndf = raw_df\ndf = df.withColumn('S3_filename_path', input_file_name()) #get original file location for data\ndf = df.withColumn(\"filename\", split(col('S3_filename_path'), \"/\").getItem(5))\ndf = df.withColumn(\"temp\", split(col(\"filename\"), \".json\").getItem(0)) \ndf = df.withColumn(\"index\", split(col(\"temp\"), \"_\").getItem(0)) \ndf = df.withColumn(\"index\", col(\"index\").cast(\"int\"))\ndf = df.withColumn(\"ticker\", split(col(\"temp\"), \"_\").getItem(1))\ndf = df.drop(\"temp\", \"filename\", \"S3_filename_path\")\n\n#handle dates\ndf = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\")) # Convert to date type \n# Extract year, month, and day \ndf = df.withColumn(\"year\", year(df[\"date\"])) \ndf = df.withColumn(\"month\", month(df[\"date\"])) \ndf = df.withColumn(\"day\", dayofmonth(df[\"date\"]))\n\n#handle marketcap to only have 2 decimals\ndf = df.withColumn(\"market_cap\", col(\"market_cap\").cast(DecimalType(20, 2)))\n\n#sort the spark df by date; this will cut writing time by over 90%!!!\ndf = df.orderBy(\"year\", \"month\", \"day\", \"index\") \ndf.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+-------------+-----+------+----+-----+---+\n|      date|   market_cap|index|ticker|year|month|day|\n+----------+-------------+-----+------+----+-----+---+\n|1998-01-02|7225245173.09|  136|   BKR|1998|    1|  2|\n|1998-01-02|7897793176.00|  151|     D|1998|    1|  2|\n|1998-01-02|3461966943.69|  206|   HUM|1998|    1|  2|\n|1998-01-02|3082839627.78|  222|   FCX|1998|    1|  2|\n|1998-01-02|6488518654.73|  294|   TMO|1998|    1|  2|\n+----------+-------------+-----+------+----+-----+---+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "3469774\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#check for duplicates\n# Check for duplicates across all columns\ntotal_rows = df.count()\ndistinct_rows = df.distinct().count()\n\nif total_rows == distinct_rows:\n    print(\"No duplicate rows found.\")\nelse:\n    print(f\"Duplicates found: {total_rows - distinct_rows} duplicate rows.\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "No duplicate rows found.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf = DynamicFrame.fromDF(df, glueContext, \"dyf\") # Convert Spark DataFrame to Glue DynamicFrame \ndyf.printSchema() # Show the schema of the DynamicFrame ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date: date\n|-- market_cap: decimal\n|-- index: int\n|-- ticker: string\n|-- year: int\n|-- month: int\n|-- day: int\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf.count()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "3469774\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Delete everything in output file first to prevent conflict errors\nimport boto3 \n\ns3 = boto3.client('s3')\n# Define the bucket and prefix (folder path)\nbucket_name = 'sp500-historical-analysis-project'\nprefix = 'PARQUET_final_company_market_cap_data/'\n\n\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix) # List all objects in the specified prefix\ndelete_keys = [{'Key': obj['Key']} for obj in response.get('Contents', [])] # Collect all object keys to delete\n# Delete all objects\nif delete_keys:\n    s3.delete_objects(Bucket=bucket_name, Delete={'Objects': delete_keys})\n\nprint(f\"Deleted {len(delete_keys)} objects from {bucket_name}/{prefix}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Deleted 0 objects from sp500-historical-analysis-project/PARQUET_final_company_market_cap_data/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# s3output = glueContext.getSink(\n#   path='s3://sp500-historical-analysis-project/PARQUET_final_company_market_cap_data/',\n#   connection_type=\"s3\",\n#   updateBehavior=\"UPDATE_IN_DATABASE\",\n#   partitionKeys=[\"year\", \"month\"],\n#   compression=\"snappy\",\n#   enableUpdateCatalog=True,\n#   transformation_ctx=\"s3output\",\n# )\n\n#try without partitions\ns3output = glueContext.getSink(\n  path='s3://sp500-historical-analysis-project/PARQUET_final_company_market_cap_data/',\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\n\ns3output.setCatalogInfo(\n  catalogDatabase=\"sp500_db_final\", catalogTableName=\"PARQUET_marketcaps\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(dyf)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f9ee64a2b30>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Stop code command",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: b0a97ea2-cbb3-4de6-8809-0f33aafd520d\nStopped session.\n",
					"output_type": "stream"
				}
			]
		}
	]
}